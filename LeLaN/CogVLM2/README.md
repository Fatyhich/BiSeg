# CogVLM2 Docker Installation and Usage Guide

This repository provides a Docker-based setup for running the CogVLM2 model. The model generates captions for images and outputs the results in a JSON file, where the image names are used as keys and the captions as values.

---

## Table of Contents
1. [Prerequisites](#prerequisites)
2. [Building the Docker Image](#building-the-docker-image)
3. [Running the Docker Container](#running-the-docker-container)
4. [Using the Model](#using-the-model)
5. [Output Format](#output-format)
6. [Example Query](#example-query)
7. [Troubleshooting](#troubleshooting)
8. [Contributing](#contributing)
9. [License](#license)

---

### Prerequisites

Before proceeding, ensure you have the following installed on your system:

- **Docker**: Install Docker from the [official website](https://www.docker.com/products/docker-desktop).
- **NVIDIA GPU (Optional)**: If you plan to use GPU acceleration, ensure you have an NVIDIA GPU and the [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html) installed.

---

### Building the Docker Image

1. Clone this repository or place your `Dockerfile` in a directory.
2. Open a terminal and navigate to the directory containing the `Dockerfile`.
3. Build the Docker image using the following command:

   ```bash
   docker build -t cogvlm2 .
   ```

   This will create a Docker image named `cogvlm2`.

---

### Running the Docker Container

To run the Docker container with the necessary configurations, execute the following command:

```bash
docker run --name cogvlm2 \
           --rm \
           -it \
           -v ~/personal/cogvlm2:/app \
           --entrypoint /bin/bash \
           --gpus all \
           cogvlm2
```

#### Explanation of Flags:
- `--name cogvlm2`: Assigns a name to the container.
- `--rm`: Automatically removes the container when it exits.
- `-it`: Runs the container interactively with a terminal session.
- `-v ~/personal/cogvlm2:/app`: Mounts your local `~/personal/cogvlm2` directory to `/app` inside the container. Replace this path with the location of your input data and output directory.
- `--entrypoint /bin/bash`: Starts a Bash shell instead of the default entrypoint.
- `--gpus all`: Enables access to all available GPUs for accelerated processing.
- `cogvlm2`: The name of the Docker image.

---

### Using the Model

Once the container is running, you can execute the CogVLM2 model by running the following command inside the container:

```bash
python3 run.py --image_folder /path/to/images --query "Your query here" --output_json /path/to/output.json --batch_size 3
```

#### Example Command:
```bash
python3 run.py --image_folder /app/flat_dataset/run_1/sample \
               --query "Provide a detailed description of the most prominent object in the scene and how to navigate towards it." \
               --output_json /app/nav_captions_sample.json \
               --batch_size 3
```

#### Explanation of Arguments:
- `--image_folder`: Path to the folder containing the images to process.
- `--query`: The instruction or prompt for the model. See the [Example Query](#example-query) section for suggestions.
- `--output_json`: Path to save the output JSON file containing the generated captions.
- `--batch_size`: Number of images processed in each batch.

Ensure that your input images are placed in the mounted directory (`~/personal/cogvlm2`), as specified in the `-v` flag during the `docker run` command.

---

### Output Format

After the model finishes processing, a JSON file will be generated in the specified output directory. The JSON file will have the following structure:

```json
{
    "image1.jpg": "A description of image1",
    "image2.jpg": "A description of image2",
    ...
}
```

- **Key**: The filename of the image.
- **Value**: The caption generated by the CogVLM2 model.

---


